{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4becdf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### Use this for inference only with jupyter notebook, download the model from the ./models folder\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import models, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ete3 import Tree\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "## add your own path\n",
    "USER_CHECKPOINTS_DIR = \"./models/results/final-fold/kfold_checkpoints\"\n",
    "TRAIN_ANNOTATIONS_CSV = './data/train.csv'\n",
    "TEST_ANNOTATIONS_CSV  = './data/test.csv'\n",
    "TREE_FILE_PATH        = './data/tree.nh'\n",
    "ROI_FOLDER_NAME = \"rois\"\n",
    "FULL_IMAGE_FOLDER_NAME = \"images\"\n",
    "MAX_DISTANCE = 12\n",
    "BATCH_SIZE   = 32\n",
    "NUM_WORKERS  = os.cpu_count() // 2 if os.cpu_count() else 2\n",
    "ROI_IMAGE_SIZE = (224, 224)\n",
    "TARGET_FULL_IMAGE_EDGE = 384\n",
    "\n",
    "SLIGHT_ROTATION_ANGLE = 10\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"[seed_everything] seed={seed} applied.\")\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def get_full_image_path_from_roi_path(roi_path: str, roi_folder_name: str = \"rois\", full_image_folder_name: str = \"images\") -> str:\n",
    "    roi_filename = os.path.basename(roi_path)\n",
    "    base_roi_name = os.path.splitext(roi_filename)[0].lstrip('_')\n",
    "    parts = base_roi_name.split('_')\n",
    "    image_id_str = parts[0] if parts[0] else parts[1]\n",
    "    if not image_id_str:\n",
    "        raise ValueError(f\"Cannot extract image_id from ROI filename '{roi_filename}'.\")\n",
    "    \n",
    "    full_image_filename = f\"{image_id_str}.png\"\n",
    "    roi_dir = os.path.dirname(roi_path)\n",
    "    path_parts = roi_dir.split(os.sep)\n",
    "    try:\n",
    "        idx_to_replace = path_parts.index(roi_folder_name)\n",
    "        path_parts[idx_to_replace] = full_image_folder_name\n",
    "        full_image_folder_path = os.sep.join(path_parts)\n",
    "    except ValueError:\n",
    "         raise ValueError(f\"ROI folder name '{roi_folder_name}' not found in ROI path '{roi_path}'.\")\n",
    "    return os.path.join(full_image_folder_path, full_image_filename)\n",
    "\n",
    "class FathomNetDataset(Dataset):\n",
    "    def __init__(self, df_subset, transform_roi, transform_full, label_encoder):\n",
    "        self.data = df_subset.reset_index(drop=True)\n",
    "        self.roi_image_paths = self.data['path'].tolist()\n",
    "        self.transform_roi = transform_roi\n",
    "        self.transform_full = transform_full\n",
    "        self.label_encoder = label_encoder\n",
    "        self.original_ids = self.data.get('annotation_id', self.data.get('id', self.data['path'])).tolist()\n",
    "        self.full_image_paths = [get_full_image_path_from_roi_path(p) for p in tqdm(self.roi_image_paths, desc=\"Deriving full image paths\")]\n",
    "\n",
    "    def __len__(self): return len(self.roi_image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        roi_img = Image.open(self.roi_image_paths[idx]).convert(\"RGB\")\n",
    "        full_img = Image.open(self.full_image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform_roi: roi_img = self.transform_roi(roi_img)\n",
    "        if self.transform_full: full_img = self.transform_full(full_img)\n",
    "        return (roi_img, full_img), self.original_ids[idx]\n",
    "\n",
    "class FathomNetHierarchicalClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes, dropout1=0.4, dropout2=0.3, hidden_dim=512, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        roi_backbone = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.DEFAULT)\n",
    "        roi_feat_dim = roi_backbone.classifier[1].in_features\n",
    "        roi_backbone.classifier = nn.Identity()\n",
    "        self.roi_backbone = roi_backbone\n",
    "        full_image_backbone = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "        full_feat_dim = full_image_backbone.classifier[1].in_features\n",
    "        full_image_backbone.classifier = nn.Identity()\n",
    "        self.full_image_backbone = full_image_backbone\n",
    "        combined_feat_dim = roi_feat_dim + full_feat_dim\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Dropout(p=dropout1),\n",
    "            nn.Linear(combined_feat_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_tuple):\n",
    "        x_roi, x_full = x_tuple\n",
    "        roi_features = self.roi_backbone(x_roi)\n",
    "        full_features = self.full_image_backbone(x_full)\n",
    "        combined_features = torch.cat((roi_features, full_features), dim=1)\n",
    "        return self.classifier_head(combined_features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(SEED)\n",
    "    print(f\"Using 2-way TTA with slight rotation angle: {SLIGHT_ROTATION_ANGLE} degrees\")\n",
    "    print(\"Using weighted arithmetic mean for ensembling.\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type == \"cuda\" and hasattr(torch, 'set_float32_matmul_precision'):\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(\"\\n\" + \"‚îÄ\"*25 + \" GLOBAL SETUP PHASE \" + \"‚îÄ\"*25)\n",
    "    train_df = pd.read_csv(TRAIN_ANNOTATIONS_CSV)\n",
    "    global_label_encoder = LabelEncoder().fit(train_df['label'].unique())\n",
    "    num_classes = len(global_label_encoder.classes_)\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    checkpoint_files = sorted(glob.glob(os.path.join(USER_CHECKPOINTS_DIR, \"**\", \"*.ckpt\"), recursive=True))\n",
    "    if not checkpoint_files:\n",
    "        raise FileNotFoundError(f\"No .ckpt model files found in '{USER_CHECKPOINTS_DIR}'.\")\n",
    "    print(f\"\\nFound {len(checkpoint_files)} model(s). Parsing scores for weighted ensembling...\")\n",
    "    models_with_scores = []\n",
    "    for ckpt_path in checkpoint_files:\n",
    "        match = re.search(r\"val_hdist=([0-9]+\\.?[0-9]*)\", os.path.basename(ckpt_path))\n",
    "        score = float(match.group(1)) if match else float(MAX_DISTANCE)\n",
    "        models_with_scores.append({'path': ckpt_path, 'score': score})\n",
    "        print(f\"  - Parsed score {score:.4f} from: {os.path.basename(ckpt_path)}\")\n",
    "    raw_scores = np.array([m['score'] for m in models_with_scores])\n",
    "    weights_np = 1.0 / (raw_scores + 1e-6)\n",
    "    model_weights = torch.tensor(weights_np / np.sum(weights_np), dtype=torch.float32, device=device)\n",
    "    print(\"\\nCalculated Model Weights:\")\n",
    "    for i, m in enumerate(models_with_scores):\n",
    "        print(f\"  Model: {os.path.basename(m['path'])}, Weight: {model_weights[i].item():.4f}\")\n",
    "    print(\"\\n\" + \"‚îÄ\"*25 + \" ENSEMBLE INFERENCE PHASE \" + \"‚îÄ\"*25)\n",
    "    val_test_transforms_roi = transforms.Compose([\n",
    "        transforms.Resize(ROI_IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    full_image_transforms = transforms.Compose([\n",
    "        transforms.Resize((TARGET_FULL_IMAGE_EDGE, TARGET_FULL_IMAGE_EDGE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    test_df = pd.read_csv(TEST_ANNOTATIONS_CSV)\n",
    "    test_ds = FathomNetDataset(test_df, val_test_transforms_roi, full_image_transforms, global_label_encoder)\n",
    "    test_loader = DataLoader(test_ds, BATCH_SIZE*2, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True, worker_init_fn=seed_worker)\n",
    "    print(f\"‚úÖ Test DataLoader initialized with {len(test_ds)} samples.\")\n",
    "    all_model_probs = []\n",
    "    all_original_ids = []\n",
    "    for model_idx, model_info in enumerate(models_with_scores):\n",
    "        print(f\"\\nInferring with model {model_idx + 1}/{len(models_with_scores)}: {os.path.basename(model_info['path'])}\")\n",
    "        model = FathomNetHierarchicalClassifier.load_from_checkpoint(\n",
    "            checkpoint_path=model_info['path'], map_location=\"cpu\"\n",
    "        ).to(device).eval()\n",
    "        model_accumulated_logits = []\n",
    "        with torch.no_grad():\n",
    "            for (roi_images, full_images), batch_ids in tqdm(test_loader, desc=f\"Predict (M {model_idx+1})\"):\n",
    "                if model_idx == 0: all_original_ids.extend(list(batch_ids))\n",
    "                roi_images, full_images = roi_images.to(device), full_images.to(device)\n",
    "                logits1 = model((roi_images, full_images))\n",
    "                roi_rot, full_rot = TF.rotate(roi_images, angle=SLIGHT_ROTATION_ANGLE), TF.rotate(full_images, angle=SLIGHT_ROTATION_ANGLE)\n",
    "                logits2 = model((roi_rot, full_rot))\n",
    "                batch_avg_tta_logits = (logits1 + logits2) / 2.0\n",
    "                model_accumulated_logits.append(batch_avg_tta_logits.cpu())\n",
    "        model_probs = F.softmax(torch.cat(model_accumulated_logits), dim=1)\n",
    "        all_model_probs.append(model_probs)\n",
    "        del model\n",
    "        if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "    print(\"\\nApplying weighted arithmetic mean to ensembled probabilities.\")\n",
    "    stacked_probs = torch.stack(all_model_probs)\n",
    "    ensembled_probs = torch.sum(stacked_probs * model_weights.cpu().view(-1, 1, 1), dim=0)\n",
    "    final_preds_indices = torch.argmax(ensembled_probs, dim=1).numpy()\n",
    "    decoded_concepts = global_label_encoder.inverse_transform(final_preds_indices)\n",
    "    submission_df = pd.DataFrame({'annotation_id': all_original_ids, 'concept_name': decoded_concepts})\n",
    "    num_ensembled_models = len(models_with_scores)\n",
    "    final_submission_path = f\"submission_weighted_arithmetic_{num_ensembled_models}models_TTA-2way_rotate{SLIGHT_ROTATION_ANGLE}.csv\"\n",
    "    submission_df.to_csv(final_submission_path, index=False)\n",
    "    print(f\"\\n‚úÖ Final submission written to {final_submission_path}; preview:\")\n",
    "    print(submission_df.head())\n",
    "    print(\"\\nüèÅ Inference script execution finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
